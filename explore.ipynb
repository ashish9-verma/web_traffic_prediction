{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "TqdmDeprecationWarning: Please use `tqdm.pandas(...)` instead of `tqdm_pandas(tqdm(...))`.\n",
      "/Library/Python/2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm_pandas(tqdm())\n",
    "import datetime\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import operator\n",
    "from scipy.stats import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train_2.csv').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = pd.read_csv('./key_trunc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"./sample_submission_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_melt = pd.read_csv(\"train_feats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-dea6371ed687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2329\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2331\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2396\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2398\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1759\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1760\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value, check)\u001b[0m\n\u001b[1;32m   3743\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3744\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3745\u001b[0;31m                     \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3746\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   3284\u001b[0m         \u001b[0mkeep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3285\u001b[0m         \u001b[0mslobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3286\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for col in train.columns[1:]:\n",
    "    train[col] = pd.to_numeric(train[col],downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_median(row, last_n_days, rows_to_consider, is_it_train):\n",
    "    if is_it_train == 1:\n",
    "        return pd.Series(pd.Series(row)[1:].rolling(min_periods=7,window=last_n_days,center=False).median()[rows_to_consider:])\n",
    "    else:\n",
    "        return pd.Series(pd.Series(row)[1:].rolling(min_periods=7,window=last_n_days,center=False).median()[rows_to_consider - 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean(row, last_n_days, rows_to_consider, is_it_train):\n",
    "    if is_it_train == 1:\n",
    "        return pd.Series(pd.Series(row)[1:].rolling(min_periods=7,window=last_n_days,center=False).mean()[rows_to_consider:]) \n",
    "    else:\n",
    "        return pd.Series(pd.Series(row)[1:].rolling(min_periods=7,window=last_n_days,center=False).mean()[rows_to_consider - 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_std(row, last_n_days, rows_to_consider, is_it_train):\n",
    "    if is_it_train == 1:\n",
    "        return pd.Series(pd.Series(row)[1:].rolling(min_periods=7,window=last_n_days,center=False).std()[rows_to_consider:])\n",
    "    else:\n",
    "        return pd.Series(pd.Series(row)[1:].rolling(min_periods=7,window=last_n_days,center=False).std()[rows_to_consider - 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_min(row, last_n_days, rows_to_consider, is_it_train):\n",
    "    if is_it_train == 1:\n",
    "        return pd.Series(pd.Series(row)[1:].rolling(min_periods=7,window=last_n_days,center=False).min()[rows_to_consider:])\n",
    "    else:\n",
    "        return pd.Series(pd.Series(row)[1:].rolling(min_periods=7,window=last_n_days,center=False).min()[rows_to_consider - 1:])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max(row, last_n_days, rows_to_consider, is_it_train):\n",
    "    if is_it_train == 1:\n",
    "        return pd.Series(pd.Series(row)[1:].rolling(min_periods=7,window=last_n_days,center=False).max()[rows_to_consider:])\n",
    "    else:\n",
    "        return pd.Series(pd.Series(row)[1:].rolling(min_periods=7,window=last_n_days,center=False).max()[rows_to_consider - 1:])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_overall(row, rows_to_consider, is_it_train):\n",
    "    if is_it_train == 1:\n",
    "        return pd.Series(pd.Series(row)[1:].mean()).repeat(-1*rows_to_consider)\n",
    "    else:\n",
    "        return pd.Series(pd.Series(row)[1:].mean()).repeat(len(row) - rows_to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_median_overall(row, rows_to_consider, is_it_train):\n",
    "    if is_it_train == 1:\n",
    "        return pd.Series(pd.Series(row)[1:].median()).repeat(-1*rows_to_consider)\n",
    "    else:\n",
    "        return pd.Series(pd.Series(row)[1:].median()).repeat(len(row) - rows_to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_mean_diff(row, last_n_days):\n",
    "#     return pd.Series(pd.rolling_mean(np.diff(pd.Series(row)), window = last_n_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_last_day_visits(row):\n",
    "#     return row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_long_stats(func_type, df, df_melt, rows_to_consider, is_it_train = None, last_n_days = None):\n",
    "\n",
    "    if func_type == \"median\":\n",
    "        rolling_stats = df.apply(get_median, last_n_days = last_n_days, rows_to_consider = rows_to_consider,is_it_train = is_it_train, axis = 1)\n",
    "        \n",
    "    elif func_type == \"mean\":\n",
    "        rolling_stats = df.apply(get_mean, last_n_days = last_n_days, rows_to_consider = rows_to_consider, is_it_train = is_it_train, axis = 1)\n",
    "    \n",
    "    elif func_type == \"std\":\n",
    "        rolling_stats = df.apply(get_std, last_n_days = last_n_days, rows_to_consider = rows_to_consider, is_it_train = is_it_train, axis = 1)\n",
    "\n",
    "    elif func_type == \"min\":\n",
    "        rolling_stats = df.apply(get_min, last_n_days = last_n_days, rows_to_consider = rows_to_consider, is_it_train = is_it_train, axis = 1)\n",
    "\n",
    "    elif func_type == \"max\":\n",
    "        rolling_stats = df.apply(get_max, last_n_days = last_n_days, rows_to_consider = rows_to_consider, is_it_train = is_it_train, axis = 1)\n",
    "\n",
    "    elif func_type == \"mean_overall\":\n",
    "        rolling_stats = df.apply(get_mean_overall, rows_to_consider = rows_to_consider, is_it_train = is_it_train, axis = 1)\n",
    "\n",
    "    elif func_type == \"median_overall\":\n",
    "        rolling_stats = df.apply(get_median_overall, rows_to_consider = rows_to_consider, is_it_train = is_it_train, axis = 1)\n",
    "        \n",
    "    rolling_stats_df = pd.concat([df.Page, rolling_stats.apply(pd.Series)], axis = 1)  \n",
    "    rolling_stats_df.columns = [\"Page\"] + list(df.columns[rows_to_consider:])\n",
    "    \n",
    "    rolling_stats_df = pd.melt(rolling_stats_df, id_vars=['Page'], var_name=\"date\", value_name=func_type + \"_\" + str(last_n_days))        \n",
    "    \n",
    "    df_melt = pd.merge(df_melt, rolling_stats_df, on = [\"Page\", \"date\"])     \n",
    "    \n",
    "    return df_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Page        date  visits  \\\n",
      "0            2NE1_zh.wikipedia.org_all-access_spider  2017-03-05    16.0   \n",
      "1             2PM_zh.wikipedia.org_all-access_spider  2017-03-05    15.0   \n",
      "2              3C_zh.wikipedia.org_all-access_spider  2017-03-05     5.0   \n",
      "3         4minute_zh.wikipedia.org_all-access_spider  2017-03-05    23.0   \n",
      "4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...  2017-03-05    12.0   \n",
      "\n",
      "     mean_30    mean_60  \n",
      "0  19.666667  25.533333  \n",
      "1  30.233333  27.900000  \n",
      "2   3.500000   3.766667  \n",
      "3  22.966667  17.450000  \n",
      "4  42.633333  45.366667  \n",
      "                                                Page        date  visits  \\\n",
      "0            2NE1_zh.wikipedia.org_all-access_spider  2017-03-05    16.0   \n",
      "1             2PM_zh.wikipedia.org_all-access_spider  2017-03-05    15.0   \n",
      "2              3C_zh.wikipedia.org_all-access_spider  2017-03-05     5.0   \n",
      "3         4minute_zh.wikipedia.org_all-access_spider  2017-03-05    23.0   \n",
      "4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...  2017-03-05    12.0   \n",
      "\n",
      "     mean_30    mean_60  median_30  median_60  \n",
      "0  19.666667  25.533333       16.0       16.0  \n",
      "1  30.233333  27.900000       24.0       20.0  \n",
      "2   3.500000   3.766667        3.5        3.0  \n",
      "3  22.966667  17.450000       19.0       15.0  \n",
      "4  42.633333  45.366667       32.5       32.5  \n",
      "                                                Page        date  visits  \\\n",
      "0            2NE1_zh.wikipedia.org_all-access_spider  2017-03-05    16.0   \n",
      "1             2PM_zh.wikipedia.org_all-access_spider  2017-03-05    15.0   \n",
      "2              3C_zh.wikipedia.org_all-access_spider  2017-03-05     5.0   \n",
      "3         4minute_zh.wikipedia.org_all-access_spider  2017-03-05    23.0   \n",
      "4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...  2017-03-05    12.0   \n",
      "\n",
      "     mean_30    mean_60  median_30  median_60     std_30     std_60  \n",
      "0  19.666667  25.533333       16.0       16.0  12.614533  22.974906  \n",
      "1  30.233333  27.900000       24.0       20.0  16.348960  18.272606  \n",
      "2   3.500000   3.766667        3.5        3.0   2.029948   2.889881  \n",
      "3  22.966667  17.450000       19.0       15.0  17.848951  14.551399  \n",
      "4  42.633333  45.366667       32.5       32.5  30.010898  40.340906  \n",
      "                                                Page        date  visits  \\\n",
      "0            2NE1_zh.wikipedia.org_all-access_spider  2017-03-05    16.0   \n",
      "1             2PM_zh.wikipedia.org_all-access_spider  2017-03-05    15.0   \n",
      "2              3C_zh.wikipedia.org_all-access_spider  2017-03-05     5.0   \n",
      "3         4minute_zh.wikipedia.org_all-access_spider  2017-03-05    23.0   \n",
      "4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...  2017-03-05    12.0   \n",
      "\n",
      "     mean_30    mean_60  median_30  median_60     std_30     std_60  min_30  \\\n",
      "0  19.666667  25.533333       16.0       16.0  12.614533  22.974906     7.0   \n",
      "1  30.233333  27.900000       24.0       20.0  16.348960  18.272606    11.0   \n",
      "2   3.500000   3.766667        3.5        3.0   2.029948   2.889881     0.0   \n",
      "3  22.966667  17.450000       19.0       15.0  17.848951  14.551399     8.0   \n",
      "4  42.633333  45.366667       32.5       32.5  30.010898  40.340906    12.0   \n",
      "\n",
      "   min_60  \n",
      "0     4.0  \n",
      "1     8.0  \n",
      "2     0.0  \n",
      "3     2.0  \n",
      "4     9.0  \n",
      "                                                Page        date  visits  \\\n",
      "0            2NE1_zh.wikipedia.org_all-access_spider  2017-03-05    16.0   \n",
      "1             2PM_zh.wikipedia.org_all-access_spider  2017-03-05    15.0   \n",
      "2              3C_zh.wikipedia.org_all-access_spider  2017-03-05     5.0   \n",
      "3         4minute_zh.wikipedia.org_all-access_spider  2017-03-05    23.0   \n",
      "4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...  2017-03-05    12.0   \n",
      "\n",
      "     mean_30    mean_60  median_30  median_60     std_30     std_60  min_30  \\\n",
      "0  19.666667  25.533333       16.0       16.0  12.614533  22.974906     7.0   \n",
      "1  30.233333  27.900000       24.0       20.0  16.348960  18.272606    11.0   \n",
      "2   3.500000   3.766667        3.5        3.0   2.029948   2.889881     0.0   \n",
      "3  22.966667  17.450000       19.0       15.0  17.848951  14.551399     8.0   \n",
      "4  42.633333  45.366667       32.5       32.5  30.010898  40.340906    12.0   \n",
      "\n",
      "   min_60  max_30  max_60  \n",
      "0     4.0    66.0   120.0  \n",
      "1     8.0    77.0    87.0  \n",
      "2     0.0     9.0    17.0  \n",
      "3     2.0   106.0   106.0  \n",
      "4     9.0   164.0   195.0  \n",
      "                                                Page        date  visits  \\\n",
      "0            2NE1_zh.wikipedia.org_all-access_spider  2017-03-05    16.0   \n",
      "1             2PM_zh.wikipedia.org_all-access_spider  2017-03-05    15.0   \n",
      "2              3C_zh.wikipedia.org_all-access_spider  2017-03-05     5.0   \n",
      "3         4minute_zh.wikipedia.org_all-access_spider  2017-03-05    23.0   \n",
      "4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...  2017-03-05    12.0   \n",
      "\n",
      "     mean_30    mean_60  median_30  median_60     std_30     std_60  min_30  \\\n",
      "0  19.666667  25.533333       16.0       16.0  12.614533  22.974906     7.0   \n",
      "1  30.233333  27.900000       24.0       20.0  16.348960  18.272606    11.0   \n",
      "2   3.500000   3.766667        3.5        3.0   2.029948   2.889881     0.0   \n",
      "3  22.966667  17.450000       19.0       15.0  17.848951  14.551399     8.0   \n",
      "4  42.633333  45.366667       32.5       32.5  30.010898  40.340906    12.0   \n",
      "\n",
      "   min_60  max_30  max_60  mean_overall_60  median_overall_60  \n",
      "0     4.0    66.0   120.0        23.238335               18.0  \n",
      "1     8.0    77.0    87.0        26.912989               19.0  \n",
      "2     0.0     9.0    17.0         4.941992                3.0  \n",
      "3     2.0   106.0   106.0        17.369483               14.0  \n",
      "4     9.0   164.0   195.0        10.761665                4.0  \n",
      "                                                Page        date  visits  \\\n",
      "0            2NE1_zh.wikipedia.org_all-access_spider  2017-03-05    16.0   \n",
      "1             2PM_zh.wikipedia.org_all-access_spider  2017-03-05    15.0   \n",
      "2              3C_zh.wikipedia.org_all-access_spider  2017-03-05     5.0   \n",
      "3         4minute_zh.wikipedia.org_all-access_spider  2017-03-05    23.0   \n",
      "4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...  2017-03-05    12.0   \n",
      "\n",
      "     mean_30    mean_60  median_30  median_60     std_30     std_60  min_30  \\\n",
      "0  19.666667  25.533333       16.0       16.0  12.614533  22.974906     7.0   \n",
      "1  30.233333  27.900000       24.0       20.0  16.348960  18.272606    11.0   \n",
      "2   3.500000   3.766667        3.5        3.0   2.029948   2.889881     0.0   \n",
      "3  22.966667  17.450000       19.0       15.0  17.848951  14.551399     8.0   \n",
      "4  42.633333  45.366667       32.5       32.5  30.010898  40.340906    12.0   \n",
      "\n",
      "   min_60  max_30  max_60  mean_overall_60  median_overall_60 date_num  \\\n",
      "0     4.0    66.0   120.0        23.238335               18.0       05   \n",
      "1     8.0    77.0    87.0        26.912989               19.0       05   \n",
      "2     0.0     9.0    17.0         4.941992                3.0       05   \n",
      "3     2.0   106.0   106.0        17.369483               14.0       05   \n",
      "4     9.0   164.0   195.0        10.761665                4.0       05   \n",
      "\n",
      "   weekday  \n",
      "0        6  \n",
      "1        6  \n",
      "2        6  \n",
      "3        6  \n",
      "4        6  \n"
     ]
    }
   ],
   "source": [
    "def generate_train_melt(train):\n",
    "    \n",
    "    train_melt = pd.melt(pd.concat([train.Page, \n",
    "                                    train.iloc[:, -180:]],\n",
    "                                    axis = 1),\n",
    "                                    id_vars=['Page'], \n",
    "                                    var_name=\"date\", value_name=\"visits\")\n",
    "\n",
    "    train_melt = get_long_stats(\"mean\", train, train_melt, rows_to_consider=-180, is_it_train = 1, last_n_days=30)\n",
    "    train_melt = get_long_stats(\"mean\", train, train_melt, rows_to_consider=-180, is_it_train = 1, last_n_days=60)\n",
    "    print train_melt.head()\n",
    "    \n",
    "    train_melt = get_long_stats(\"median\", train, train_melt, rows_to_consider=-180, is_it_train = 1, last_n_days=30)\n",
    "    train_melt = get_long_stats(\"median\", train, train_melt, rows_to_consider=-180, is_it_train = 1, last_n_days=60)\n",
    "    print train_melt.head()\n",
    "\n",
    "    train_melt = get_long_stats(\"std\", train, train_melt, rows_to_consider=-180, is_it_train = 1, last_n_days=30)\n",
    "    train_melt = get_long_stats(\"std\", train, train_melt, rows_to_consider=-180, is_it_train = 1, last_n_days=60)\n",
    "    print train_melt.head()\n",
    "\n",
    "    train_melt = get_long_stats(\"min\", train, train_melt, rows_to_consider=-180, is_it_train = 1, last_n_days=30)\n",
    "    train_melt = get_long_stats(\"min\", train, train_melt, rows_to_consider=-180, is_it_train = 1, last_n_days=60)\n",
    "    print train_melt.head()\n",
    "\n",
    "    train_melt = get_long_stats(\"max\", train, train_melt, rows_to_consider=-180, is_it_train = 1, last_n_days=30)\n",
    "    train_melt = get_long_stats(\"max\", train, train_melt, rows_to_consider=-180, is_it_train = 1, last_n_days=60)\n",
    "    print train_melt.head()\n",
    "\n",
    "    train_melt = get_long_stats(\"mean_overall\", train, train_melt, rows_to_consider=-180, is_it_train = 1, last_n_days=60)\n",
    "    train_melt = get_long_stats(\"median_overall\", train, train_melt, rows_to_consider=-180, is_it_train = 1, last_n_days=60)\n",
    "    print train_melt.head()\n",
    "        \n",
    "    train_melt[\"date_num\"] = train_melt.apply(get_date, axis = 1)\n",
    "    train_melt[\"weekday\"] = train_melt.apply(get_weekday, axis = 1)\n",
    "    print train_melt.head()\n",
    "    \n",
    "    return train_melt \n",
    "\n",
    "def get_lang_features(train_melt):\n",
    "    \n",
    "    train_melt = pd.merge(train_melt, pd.concat([train.Page, train.progress_apply(get_language, axis = 1)], axis = 1), on = \"Page\")\n",
    "    train_melt.columns = ['Page', 'date', 'visits', 'mean_30', 'mean_60', 'median_30', 'median_60', 'std_30', 'std_60', 'min_30', 'min_60', 'max_30', 'max_60', 'mean_overall', 'median_overall', 'date_num', 'weekday', 'language']\n",
    "    \n",
    "    lang_date_melt = train_melt[[\"language\", \"date\", \"mean_30\", \"mean_60\", \"median_30\", \"median_60\", \"min_30\", \"min_60\", \"max_30\", \"max_60\"]]\n",
    "    \n",
    "    lang_feats = pd.DataFrame(lang_date_melt.groupby([\"language\", \"date\"], as_index = False).aggregate(np.median))\n",
    "\n",
    "    lang_feats.columns = [\"language\", \"date\", \"lang_mean_30_median\", \"lang_mean_60_median\", \"lang_median_30_median\", \"lang_median_60_median\", \"lang_min_30_median\", \"lang_min_60_median\", \"lang_max_30_median\", \"lang_max_60_median\"]\n",
    "        \n",
    "    train_melt = pd.merge(train_melt, lang_feats, on = [\"language\", \"date\"])\n",
    "    \n",
    "    return train_melt\n",
    "\n",
    "def get_weekday_features(train_melt):\n",
    "    \n",
    "    weekday_page_melt = train_melt[[\"weekday\", \"Page\", \"visits\"]]\n",
    "    \n",
    "    weekday_mean_feats = pd.DataFrame(weekday_page_melt.groupby([\"weekday\", \"Page\"], as_index = False).aggregate(np.mean))\n",
    "    weekday_mean_feats.columns = [\"weekday\", \"Page\", \"weekday_mean_visits\"]\n",
    "    \n",
    "    weekday_median_feats = pd.DataFrame(weekday_page_melt.groupby([\"weekday\", \"Page\"], as_index = False).aggregate(np.median))\n",
    "    weekday_median_feats.columns = [\"weekday\", \"Page\", \"weekday_median_visits\"]\n",
    "    \n",
    "    weekday_feats = pd.merge(weekday_mean_feats, weekday_median_feats, on = [\"weekday\", \"Page\"])\n",
    "        \n",
    "    train_melt = pd.merge(train_melt, weekday_feats, on = [\"weekday\", \"Page\"])\n",
    "    \n",
    "    return train_melt\n",
    "    \n",
    "# def get_lang_mean(train_melt):\n",
    "    \n",
    "#     language_page_melt = train_melt[[\"language\", \"date\", \"visits\"]]\n",
    "    \n",
    "#     language_feats = pd.DataFrame(language_page_melt.groupby([\"language\", \"date\"], as_index = False).aggregate(np.mean))\n",
    "\n",
    "#     language_feats.columns = [\"language\", \"date\", \"lang_mean_visits\"]\n",
    "        \n",
    "#     train_melt = pd.merge(train_melt, language_feats, on = [\"language\", \"date\"])\n",
    "    \n",
    "#     return train_melt\n",
    "\n",
    "train_melt = generate_train_melt(train)\n",
    "# train_melt = get_lang_features(train_melt)\n",
    "train_melt = get_weekday_features(train_melt)\n",
    "train_melt.to_csv(\"train_feats.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_source(row):\n",
    "    return row.Page.splitgit[\".\"][-1].split(\"_\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_language(row):\n",
    "    try:\n",
    "        return row.Page.split(\".\")[0].split(\"_\")[-1]    \n",
    "    except:\n",
    "        return \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_month(row):\n",
    "    return row.date.split(\"-\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_date(row):\n",
    "    return row.date.split(\"-\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weekday(row):\n",
    "    return datetime.datetime.strptime(row.date, '%Y-%m-%d').date().weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_language_mean(row):\n",
    "# lang_count = pd.Series(train_melt.groupby([\"language\"])[\"Page\"].count()).sort_values(axis = 0, ascending = False)\n",
    "# print lang_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lang_features_test(test_melt):\n",
    "    \n",
    "    test_melt = pd.merge(test_melt, pd.concat([overall_df.Page, overall_df.progress_apply(get_language, axis = 1)], axis = 1), on = \"Page\")\n",
    "    test_melt.columns = ['Page', 'date', 'visits', 'mean_30', 'mean_60', 'median_30', 'median_60', 'std_30', 'std_60', 'min_30', 'min_60', 'max_30', 'max_60', 'mean_overall', 'median_overall', 'date_num', 'weekday', 'language']\n",
    "    \n",
    "    lang_date_melt = test_melt[[\"language\", \"date\", \"mean_30\", \"mean_60\", \"median_30\", \"median_60\", \"min_30\", \"min_60\", \"max_30\", \"max_60\"]]\n",
    "    \n",
    "    lang_feats = pd.DataFrame(lang_date_melt.groupby([\"language\", \"date\"], as_index = False).aggregate(np.median))\n",
    "    \n",
    "    lang_feats.columns = [\"language\", \"date\", \"lang_mean_30_median\", \"lang_mean_60_median\", \"lang_median_30_median\", \"lang_median_60_median\", \"lang_min_30_median\", \"lang_min_60_median\", \"lang_max_30_median\", \"lang_max_60_median\"]\n",
    "        \n",
    "    test_melt = pd.merge(test_melt, lang_feats, on = [\"language\", \"date\"])\n",
    "    \n",
    "    return test_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weekday_features_test(train_melt, test_melt):\n",
    "    \n",
    "    weekday_page_melt = train_melt[[\"weekday\", \"Page\", \"visits\"]]\n",
    "    \n",
    "    weekday_mean_feats = pd.DataFrame(weekday_page_melt.groupby([\"weekday\", \"Page\"], as_index = False).aggregate(np.mean))\n",
    "    weekday_mean_feats.columns = [\"weekday\", \"Page\", \"weekday_mean_visits\"]\n",
    "    \n",
    "    weekday_median_feats = pd.DataFrame(weekday_page_melt.groupby([\"weekday\", \"Page\"], as_index = False).aggregate(np.median))\n",
    "    weekday_median_feats.columns = [\"weekday\", \"Page\", \"weekday_median_visits\"]\n",
    "    \n",
    "    weekday_feats = pd.merge(weekday_mean_feats, weekday_median_feats, on = [\"weekday\", \"Page\"])\n",
    "        \n",
    "    test_melt = pd.merge(test_melt, weekday_feats, on = [\"weekday\", \"Page\"], how=\"left\")\n",
    "    \n",
    "    return test_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_last_day_visits_test(row, last_index):\n",
    "    \n",
    "#     return row.iloc[last_index - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smape(y_pred, y_true):\n",
    "        \n",
    "    y_true = y_true.get_label()\n",
    "    y_pred = np.array(y_pred)\n",
    "        \n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "\n",
    "    return \"smape\", np.nanmean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_xgb(train, label_train, valid = None, label_valid = None):\n",
    "\n",
    "    # Set our parameters for xgboost\n",
    "    params = {}\n",
    "    params['objective'] = 'reg:linear'\n",
    "    params['eta'] = 0.05\n",
    "    params['max_depth'] = 7\n",
    "    params['silent'] = 1\n",
    "    params['min_child_weight'] = 0\n",
    "    params['subsample'] = 1\n",
    "    params['colsample_bytree'] = 1\n",
    "    params['nthread'] = 13\n",
    "    params['gamma'] = 0\n",
    "    params['max_delta_step'] = 0\n",
    "    \n",
    "    d_train = xgb.DMatrix(train, label=label_train)\n",
    "    \n",
    "    if valid is not None:\n",
    "        d_valid = xgb.DMatrix(valid, label=label_valid)\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'validation')]\n",
    "    else:\n",
    "        watchlist = [(d_train, 'train')]\n",
    "        \n",
    "    bst = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=10, verbose_eval=10, feval=smape, maximize = False)\n",
    "\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# label_train_melt = np.log1p(train_melt[\"visits\"])\n",
    "label_train_melt = train_melt[\"visits\"]\n",
    "\n",
    "# label_train_melt = boxcox(np.array(label_train_melt), lmbda=0.094) #lambda found from default boxcox on outlier-stripped data\n",
    "\n",
    "# label_train_melt = [0] + np.diff(train_melt[\"visits\"]).tolist()\n",
    "\n",
    "train_melt.drop(\"visits\", axis = 1, inplace = True)\n",
    "train_melt.drop(\"date\", axis = 1, inplace = True)\n",
    "# train_melt.drop(\"language\", axis = 1, inplace = True)\n",
    "train_melt.drop(\"Page\", axis = 1, inplace = True)\n",
    "\n",
    "train_melt.date_num = pd.to_numeric(train_melt.date_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_melt.drop(\"date_num\", axis = 1, inplace=True)\n",
    "train_melt.drop(\"weekday\", axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:63660.4\ttrain-smape:161.974\n",
      "Multiple eval metrics have been passed: 'train-smape' will be used for early stopping.\n",
      "\n",
      "Will train until train-smape hasn't improved in 10 rounds.\n",
      "[10]\ttrain-rmse:48434.9\ttrain-smape:78.3514\n",
      "[20]\ttrain-rmse:41524.9\ttrain-smape:55.2278\n",
      "[30]\ttrain-rmse:38689.9\ttrain-smape:49.3566\n",
      "[40]\ttrain-rmse:37594.9\ttrain-smape:48.9122\n",
      "Stopping. Best iteration:\n",
      "[32]\ttrain-rmse:38379.2\ttrain-smape:48.5358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x_train, x_valid, label_train, label_valid = train_test_split(train_melt, label_train_melt, test_size=0.2, random_state=4242)\n",
    "# bst = run_xgb(x_train, label_train, x_valid, label_valid)\n",
    "\n",
    "bst = run_xgb(train_melt, label_train_melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  feature    fscore\n",
      "0                 mean_60  0.003197\n",
      "1                  min_60  0.023181\n",
      "2                  std_60  0.026379\n",
      "3       median_overall_60  0.040368\n",
      "4                  max_60  0.044365\n",
      "5         mean_overall_60  0.045564\n",
      "6                  std_30  0.053957\n",
      "7               median_30  0.059153\n",
      "8                  max_30  0.062750\n",
      "9                 mean_30  0.079936\n",
      "10              median_60  0.090727\n",
      "11                 min_30  0.115907\n",
      "12  weekday_median_visits  0.127498\n",
      "13    weekday_mean_visits  0.227018\n"
     ]
    }
   ],
   "source": [
    "importance = bst.get_fscore()\n",
    "\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "df_imp = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "df_imp['fscore'] = df_imp['fscore']/df_imp['fscore'].sum()\n",
    "\n",
    "print df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "# x_train, x_valid, label_train, label_valid = train_test_split(train_melt, label_train_melt, test_size=0.2, random_state=4242)\n",
    "# # x_train = x_train.fillna(0)\n",
    "# # x_valid = x_valid.fillna(0)\n",
    "# reg.fit(x_train, label_train)\n",
    "\n",
    "reg.fit(train_melt, label_train_melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction = reg.predict(x_valid)\n",
    "prediction = reg.predict(test_df[[\"mean_30\",\"mean_60\",\"median_30\",\"median_60\",\"date_num\", \"weekday\", \"lang_mean_30_mean\", \"lang_mean_60_mean\", \"lang_median_30_mean\", \"lang_median_60_mean\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.106730814\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array(label_valid)\n",
    "y_pred = np.array(prediction)\n",
    "\n",
    "denominator = (np.abs(y_true) + np.abs(y_pred)) / 200.0\n",
    "diff = np.abs(y_true - y_pred) / denominator\n",
    "diff[denominator == 0] = 0.0\n",
    "\n",
    "print np.nanmean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preds(test_df):\n",
    "    \n",
    "    d_test = xgb.DMatrix(test_df)\n",
    "    p_test = bst.predict(d_test)\n",
    "    \n",
    "    return pd.Series(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_overall_df():\n",
    "    \n",
    "    key_wide = pd.DataFrame()\n",
    "    key_wide[\"page\"] = np.unique(key.trunc_page).tolist()\n",
    "    for i in np.unique(key.date).tolist():\n",
    "        key_wide[i] = np.nan\n",
    "\n",
    "    overall_df = pd.merge(train, key_wide, left_on=\"Page\", right_on = \"page\", how = \"inner\")\n",
    "    overall_df.drop(\"page\", axis = 1, inplace = True)\n",
    "    \n",
    "    return overall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_melt(overall_df):\n",
    "        \n",
    "    pred_index = overall_df.columns.tolist().index('2017-09-13') \n",
    "    \n",
    "    test_melt = pd.melt(pd.concat([overall_df.Page, overall_df.iloc[:, pred_index:]],axis = 1),\n",
    "                                id_vars=['Page'], \n",
    "                                var_name=\"date\", value_name=\"visits\")\n",
    "    \n",
    "    test_melt = get_long_stats(\"mean\", overall_df, test_melt, pred_index, is_it_train=0, last_n_days = 30)\n",
    "    test_melt = get_long_stats(\"mean\", overall_df, test_melt, pred_index, is_it_train=0, last_n_days = 60)\n",
    "\n",
    "    test_melt = get_long_stats(\"median\", overall_df, test_melt, pred_index, is_it_train=0, last_n_days = 30)\n",
    "    test_melt = get_long_stats(\"median\", overall_df, test_melt, pred_index, is_it_train=0, last_n_days = 60)\n",
    "\n",
    "    test_melt = get_long_stats(\"std\", overall_df, test_melt, pred_index, is_it_train=0, last_n_days = 30)\n",
    "    test_melt = get_long_stats(\"std\", overall_df, test_melt, pred_index, is_it_train=0, last_n_days = 60)\n",
    "\n",
    "    test_melt = get_long_stats(\"min\", overall_df, test_melt, pred_index, is_it_train=0, last_n_days = 30)\n",
    "    test_melt = get_long_stats(\"min\", overall_df, test_melt, pred_index, is_it_train=0, last_n_days = 60)\n",
    "\n",
    "    test_melt = get_long_stats(\"max\", overall_df, test_melt, pred_index, is_it_train=0, last_n_days = 30)\n",
    "    test_melt = get_long_stats(\"max\", overall_df, test_melt, pred_index, is_it_train=0, last_n_days = 60)\n",
    "\n",
    "    test_melt = get_long_stats(\"mean_overall\", overall_df, test_melt, rows_to_consider = pred_index, is_it_train=0, last_n_days = 60)\n",
    "    test_melt = get_long_stats(\"median_overall\", overall_df, test_melt, rows_to_consider = pred_index, is_it_train=0, last_n_days = 60)\n",
    "    \n",
    "    test_melt[\"date_num\"] = test_melt.apply(get_date, axis = 1)\n",
    "    test_melt[\"weekday\"] = test_melt.apply(get_weekday, axis = 1)\n",
    "\n",
    "    return test_melt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Page' 'visits'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-09f54e18a870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# test_melt = get_lang_features_test(test_melt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_melt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_weekday_features_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_melt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_melt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_melt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"visits\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-b6daa2856dc3>\u001b[0m in \u001b[0;36mget_weekday_features_test\u001b[0;34m(train_melt, test_melt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_weekday_features_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_melt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_melt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mweekday_page_melt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_melt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weekday\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Page\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"visits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mweekday_mean_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweekday_page_melt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weekday\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Page\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Page' 'visits'] not in index\""
     ]
    }
   ],
   "source": [
    "# overall_df = get_overall_df()\n",
    "\n",
    "test_melt = generate_test_melt(overall_df)\n",
    "\n",
    "# test_melt = get_lang_features_test(test_melt)\n",
    "\n",
    "test_melt = get_weekday_features_test(train_melt, test_melt)\n",
    "\n",
    "test_melt.drop(\"visits\", axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_30\n",
      "mean_60\n",
      "median_30\n",
      "median_60\n",
      "std_30\n",
      "std_60\n",
      "min_30\n",
      "min_60\n",
      "max_30\n",
      "max_60\n",
      "mean_overall_60\n",
      "median_overall_60\n",
      "weekday_mean_visits\n",
      "weekday_median_visits\n"
     ]
    }
   ],
   "source": [
    "last_rows_df = pd.DataFrame(np.unique(test_melt.Page))\n",
    "last_rows_df.columns = [\"Page\"]\n",
    "\n",
    "for i in test_melt.columns.tolist():\n",
    "\n",
    "    if i not in [\"Page\", \"date\", \"language\", \"date_num\", \"weekday\"]:\n",
    "        \n",
    "        print i\n",
    "        test_melt['date'] = pd.to_datetime(test_melt['date'])\n",
    "\n",
    "        sub_test_melt = test_melt[[\"Page\", \"date\", i]]\n",
    "\n",
    "        non_na_sub = sub_test_melt[np.isfinite(sub_test_melt[i])]\n",
    "        last_date = non_na_sub.loc[non_na_sub['date'].idxmax()][\"date\"]\n",
    "\n",
    "        last_rows = non_na_sub[non_na_sub[\"date\"] == last_date][[\"Page\", i]]\n",
    "        last_rows_df = pd.merge(last_rows_df, last_rows, on=\"Page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_melt = pd.merge(test_melt, last_rows_df, on = \"Page\")\n",
    "\n",
    "for i in train_melt.columns.tolist():\n",
    "\n",
    "    if i not in [\"Page\", \"date\", \"language\", \"date_num\", \"weekday\", \"visits\"]:\n",
    "\n",
    "        test_melt[i + \"_x\"].fillna(test_melt[i + \"_y\"], inplace = True)\n",
    "        test_melt[i] = test_melt[i + \"_x\"]\n",
    "\n",
    "        test_melt.drop(i + \"_x\", axis = 1, inplace=True)\n",
    "        test_melt.drop(i + \"_y\", axis = 1, inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_melt.to_csv(\"test_feats.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_melt = pd.read_csv(\"test_feats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_melt.date_num = pd.to_numeric(test_melt.date_num)\n",
    "prediction = get_preds(test_melt[train_melt.columns.tolist()])\n",
    "\n",
    "# prediction = invboxcox(get_preds(test_df[[\"mean_30\",\"mean_60\",\"median_30\",\"median_60\",\"date_num\", \"weekday\"]]), ld = 0.094)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# pred_index = overall_df.columns.tolist().index(\"2017-01-01\")\n",
    "\n",
    "# pred_df = overall_df.iloc[:, pred_index:]\n",
    "# pred_df[\"Page\"] = overall_df.Page\n",
    "\n",
    "# pred_df_melt = pd.melt(pred_df, id_vars=['Page'], var_name=\"date\", value_name=\"Visits\")        \n",
    "\n",
    "test_melt[\"Visits\"] = np.rint(prediction)\n",
    "# pred_df = pd.concat([test_melt[[\"Page\", \"date\"]], prediction], axis = 1)\n",
    "pred_df = test_melt[[\"Page\", \"date\", \"Visits\"]]\n",
    "\n",
    "pred_df['date'] = pd.to_datetime(pred_df['date'])\n",
    "key['date'] = pd.to_datetime(key['date'])\n",
    "\n",
    "pred_id_visits = pd.merge(pred_df, key, left_on=[\"Page\", \"date\"], right_on=[\"trunc_page\", \"date\"], how=\"inner\")[[\"Id\", \"Visits\"]]\n",
    "\n",
    "pred_id_visits.to_csv(\"pred_xgb_stage2_2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/scipy/stats/morestats.py:516: RuntimeWarning: divide by zero encountered in log\n",
      "  y = where(lmbda == 0, log(x), (x**lmbda - 1)/lmbda)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import *\n",
    "# bc_label, lambda_val = boxcox(np.array([i for i in label_train_melt if i > 4 and i < 1871])) #Outlier 10% to 90% only\n",
    "# bc_label, lambda_val = boxcox(np.array([i for i in label_train_melt]))\n",
    "\n",
    "bc_label = boxcox(np.array(label_train_melt), lmbda=0.094)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def invboxcox(y,ld):\n",
    "    if ld == 0:\n",
    "        return(np.exp(y))\n",
    "    else:\n",
    "        return(np.exp(np.log(ld*y+1)/ld))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 18.,  11.,   3., ...,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.percentile(label_train_melt, 90)\n",
    "# np.percentile(label_train_melt, 10)\n",
    "# len([i for i in label_train_melt if i > 4 and i < 1871])\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(bc_label.tolist())\n",
    "# plt.show()\n",
    "\n",
    "# bc_label\n",
    "\n",
    "# import scipy.special.inv_boxcox\n",
    "invboxcox(bc_label, ld = 0.094)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
